{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912724ab",
   "metadata": {},
   "source": [
<<<<<<< HEAD
=======
    "\n",
>>>>>>> ad61c52ac3f6f5a024febbc4d780552266b76082
    "## 深层原因分析\n",
    "\n",
    "用 **p(w_O|w_I) = exp(v'_O^T · v_I) / Σ exp(v'_w^T · v_I)** 这个形式有多个深刻的理由：\n",
    "\n",
    "### 1. **内积的几何直觉**\n",
    "- **内积 = 相似性度量**：v'_O^T · v_I 测量两个向量的夹角\n",
    "- **夹角越小 → 内积越大 → 词语越相似**\n",
    "- 这符合我们的直觉：相似的词应该在高维空间中靠近\n",
    "\n",
    "### 2. **指数函数的数学优美性**\n",
    "\n",
    "#### **A. 最大熵原理**\n",
    "- 在约束条件 E[f(x)] = μ 下，熵最大的分布是指数族：**p(x) ∝ exp(θ·f(x))**\n",
    "- Word2Vec中，特征函数就是内积：f(w_O, w_I) = v'_O^T · v_I\n",
    "- 所以最优分布自然是 exp(内积) 的形式\n",
    "\n",
    "#### **B. 单调性与放大效应**\n",
    "```python\n",
    "# 原始相似性: [0.1, 0.3, 0.8, 0.9]\n",
    "# 指数变换后: [1.11, 1.35, 2.23, 2.46]  \n",
    "# 差异被适度放大，有利于区分\n",
    "```\n",
    "\n",
    "### 3. **训练的实际好处**\n",
    "\n",
    "#### **A. 梯度性质优秀**\n",
    "- exp函数的导数等于自身：d/dx exp(x) = exp(x)\n",
    "- softmax的梯度形式简洁：∇ = p_i - δ_i（预测概率 - 真实标签）\n",
    "\n",
    "#### **B. 数值稳定性**\n",
    "- 相比直接归一化，softmax更稳定\n",
    "- 可以通过减去最大值来避免数值溢出\n",
    "\n",
    "### 4. **为什么不用其他函数？**\n",
    "\n",
    "| 函数类型 | 问题 |\n",
    "|---------|------|\n",
    "| 线性归一化 | 可能产生负概率，没有放大效应 |\n",
    "| ReLU | 不可微，梯度信息丢失 |\n",
    "| 平方函数 | 梯度爆炸，训练不稳定 |\n",
    "| Sigmoid | 饱和问题，梯度消失 |\n",
    "| **指数函数** | ✓ 完美满足所有需求 |\n",
    "\n",
    "### 5. **信息论视角**\n",
    "- **交叉熵损失** = -log p(正确答案)\n",
    "- 指数形式使得损失函数简洁：loss = -log(exp(s_正确)/Σexp(s_i))\n",
    "- 这与最大似然估计完全一致\n",
    "\n",
    "### 6. **生物学类比**\n",
    "- 类似神经元的激活：刺激越强，响应越强\n",
    "- 但响应是非线性的，符合生物直觉\n",
    "\n",
    "## 核心洞察\n",
    "\n",
    "**这不是随意的设计选择，而是多个优雅原理的汇合点：**\n",
    "\n",
    "1. **几何直觉**：内积测量相似性\n",
    "2. **信息论**：最大熵原理指向指数分布  \n",
    "3. **优化理论**：梯度性质优秀\n",
    "4. **数值计算**：稳定可靠\n",
    "5. **生物启发**：非线性激活模式\n",
    "\n",
    "所以 Word2Vec 的概率公式不仅仅是\"工程技巧\"，而是**数学美学与实用性的完美结合**！这也解释了为什么这个简单的公式能在如此多的场景下工作得如此出色。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5b859b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1. 内积作为相似性度量 ===\n",
      "中心词: king\n",
      "中心词向量: tensor([0.8000, 0.6000, 0.1000])\n",
      "\n",
      "与其他词的相似性:\n",
      "queen   : 内积=1.000, 余弦相似性=0.985\n",
      "man     : 内积=0.900, 余弦相似性=0.944\n",
      "woman   : 内积=0.970, 余弦相似性=0.960\n",
      "cat     : 内积=0.310, 余弦相似性=0.333\n",
      "dog     : 内积=0.440, 余弦相似性=0.499\n",
      "\n",
      "=== 2. 指数函数的作用 ===\n",
      "原始内积分数 -> 指数变换 -> 归一化概率\n",
      "--------------------------------------------------\n",
      "irrelevant  :  -2.00 ->     0.14 -> 0.0100\n",
      "somewhat    :  -0.50 ->     0.61 -> 0.0449\n",
      "neutral     :   0.00 ->     1.00 -> 0.0741\n",
      "related     :   0.50 ->     1.65 -> 0.1221\n",
      "similar     :   1.00 ->     2.72 -> 0.2014\n",
      "very_similar:   2.00 ->     7.39 -> 0.5474\n",
      "\n",
      "=== 3. 为什么选择指数函数？ ===\n",
      "指数函数的优势:\n",
      "1. 单调递增：相似性越高，概率越大\n",
      "2. 非负性：exp(x) > 0，适合概率分布\n",
      "3. 放大差异：小差异被放大，有利于区分\n",
      "4. 数学优美：与最大熵原理一致\n",
      "5. 梯度性质：导数等于自身，训练稳定\n",
      "\n",
      "=== 4. 最大熵原理 ===\n",
      "最大熵原理：在满足约束条件的前提下，选择熵最大的分布\n",
      "约束条件：E[f(x)] = μ (特征期望等于观测值)\n",
      "结果：最优分布是指数族分布 p(x) ∝ exp(θ·f(x))\n",
      "\n",
      "在Word2Vec中:\n",
      "- 特征函数 f(w_O, w_I) = v'_O^T * v_I (内积特征)\n",
      "- 参数 θ = 1\n",
      "- 因此 p(w_O|w_I) ∝ exp(v'_O^T * v_I)\n",
      "\n",
      "=== 5. 实际训练中的好处 ===\n",
      "训练前的概率分布:\n",
      "词 0: p = 0.1706\n",
      "词 1: p = 0.1699\n",
      "词 2: p = 0.1727\n",
      "词 3: p = 0.1591\n",
      "词 4: p = 0.1637\n",
      "词 5: p = 0.1639\n",
      "\n",
      "训练后的概率分布 (目标词是 1):\n",
      "词 0: p = 0.1703 (变化: -0.0003)\n",
      "词 1: p = 0.1715 (变化: +0.0016)\n",
      "词 2: p = 0.1724 (变化: -0.0003)\n",
      "词 3: p = 0.1588 (变化: -0.0003)\n",
      "词 4: p = 0.1634 (变化: -0.0003)\n",
      "词 5: p = 0.1636 (变化: -0.0003)\n",
      "\n",
      "=== 6. 如果用其他函数会怎样？ ===\n",
      "不同概率化方法的结果:\n",
      "原始相似性: [0.1 0.3 0.8 0.2 0.9 0.4]\n",
      "----------------------------------------\n",
      "Exponential (Word2Vec): [0.11216137 0.13699421 0.22586526 0.12395748 0.24961971 0.15140201]\n",
      "Direct normalization: [0.03703704 0.11111111 0.2962963  0.07407407 0.3333333  0.14814815]\n",
      "Squared             : [0.00571429 0.05142858 0.3657143  0.02285714 0.4628571  0.09142858]\n",
      "Linear transformed  : [0.06875888 0.10257619 0.27883098 0.08398227 0.3405649  0.12528683]\n",
      "\n",
      "指数函数的独特性:\n",
      "- 保持单调性：相似性排序不变\n",
      "- 放大差异：高相似性词获得更高概率\n",
      "- 数学性质：梯度计算简单，训练稳定\n",
      "- 理论基础：最大熵原理的自然结果\n",
      "\n",
      "=== 几何直觉 ===\n",
      "向量间的几何关系:\n",
      "B(30°): 内积=0.800, 夹角=36.9°\n",
      "C(90°): 内积=0.000, 夹角=90.0°\n",
      "D(135°): 内积=-0.600, 夹角=126.9°\n",
      "\n",
      "几何直觉:\n",
      "- 内积 = |a| * |b| * cos(θ)\n",
      "- 夹角越小，内积越大，相似性越高\n",
      "- 指数函数将这种相似性转化为概率\n",
      "\n",
      "=== 总结 ===\n",
      "p(w_O|w_I) = exp(v'_O^T * v_I) / Σ exp(v'_w^T * v_I) 的深层意义:\n",
      "\n",
      "1. 内积 v'_O^T * v_I:\n",
      "   - 几何意义：测量向量夹角，夹角越小越相似\n",
      "   - 语言学意义：捕获词汇间的语义相似性\n",
      "\n",
      "2. 指数函数 exp(·):\n",
      "   - 数学意义：最大熵原理的自然结果\n",
      "   - 实用意义：放大相似性差异，非负化\n",
      "   - 训练意义：梯度性质好，数值稳定\n",
      "\n",
      "3. 整体 softmax:\n",
      "   - 归一化：确保是有效的概率分布\n",
      "   - 竞争性：所有词竞争概率质量\n",
      "   - 可微性：支持端到端梯度训练\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import seaborn as sns\n",
    "\n",
    "class ProbabilityIntuitionDemo:\n",
    "    \"\"\"\n",
    "    演示为什么用 exp(v'_O^T * v_I) 形式的概率分布\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embedding_dim = 3  # 使用3维便于可视化\n",
    "        \n",
    "    def demonstrate_inner_product_similarity(self):\n",
    "        \"\"\"\n",
    "        演示内积作为相似性度量的直觉\n",
    "        \"\"\"\n",
    "        print(\"=== 1. 内积作为相似性度量 ===\")\n",
    "        \n",
    "        # 创建一些示例词向量\n",
    "        # 假设这些是训练后的词嵌入\n",
    "        vectors = {\n",
    "            'king': torch.tensor([0.8, 0.6, 0.1]),\n",
    "            'queen': torch.tensor([0.7, 0.7, 0.2]),\n",
    "            'man': torch.tensor([0.9, 0.3, 0.0]),\n",
    "            'woman': torch.tensor([0.6, 0.8, 0.1]),\n",
    "            'cat': torch.tensor([0.2, 0.1, 0.9]),\n",
    "            'dog': torch.tensor([0.3, 0.2, 0.8])\n",
    "        }\n",
    "        \n",
    "        center_word = 'king'\n",
    "        center_vec = vectors[center_word]\n",
    "        \n",
    "        print(f\"中心词: {center_word}\")\n",
    "        print(f\"中心词向量: {center_vec}\")\n",
    "        print(\"\\n与其他词的相似性:\")\n",
    "        \n",
    "        similarities = {}\n",
    "        for word, vec in vectors.items():\n",
    "            if word != center_word:\n",
    "                # 内积相似性\n",
    "                dot_product = torch.dot(center_vec, vec).item()\n",
    "                # 余弦相似性（标准化后的内积）\n",
    "                cos_sim = F.cosine_similarity(center_vec.unsqueeze(0), vec.unsqueeze(0)).item()\n",
    "                \n",
    "                similarities[word] = dot_product\n",
    "                print(f\"{word:8}: 内积={dot_product:.3f}, 余弦相似性={cos_sim:.3f}\")\n",
    "        \n",
    "        return similarities\n",
    "    \n",
    "    def demonstrate_exponential_transformation(self):\n",
    "        \"\"\"\n",
    "        演示指数函数的作用\n",
    "        \"\"\"\n",
    "        print(\"\\n=== 2. 指数函数的作用 ===\")\n",
    "        \n",
    "        # 模拟一些内积分数\n",
    "        raw_scores = torch.tensor([-2.0, -0.5, 0.0, 0.5, 1.0, 2.0])\n",
    "        words = ['irrelevant', 'somewhat', 'neutral', 'related', 'similar', 'very_similar']\n",
    "        \n",
    "        print(\"原始内积分数 -> 指数变换 -> 归一化概率\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # 应用指数函数\n",
    "        exp_scores = torch.exp(raw_scores)\n",
    "        # softmax归一化\n",
    "        probabilities = F.softmax(raw_scores, dim=0)\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "            print(f\"{word:12}: {raw_scores[i]:6.2f} -> {exp_scores[i]:8.2f} -> {probabilities[i]:6.4f}\")\n",
    "        \n",
    "        # 可视化指数函数的效果\n",
    "        x = np.linspace(-3, 3, 100)\n",
    "        y_linear = x\n",
    "        y_exp = np.exp(x)\n",
    "        y_softmax_like = np.exp(x) / np.sum(np.exp(x))\n",
    "        \n",
    "        return raw_scores, exp_scores, probabilities\n",
    "    \n",
    "    def demonstrate_why_exponential(self):\n",
    "        \"\"\"\n",
    "        解释为什么选择指数函数而不是其他函数\n",
    "        \"\"\"\n",
    "        print(\"\\n=== 3. 为什么选择指数函数？ ===\")\n",
    "        \n",
    "        # 比较不同的激活函数\n",
    "        x = torch.linspace(-3, 3, 100)\n",
    "        \n",
    "        functions = {\n",
    "            'Linear': x,\n",
    "            'ReLU': F.relu(x),\n",
    "            'Sigmoid': torch.sigmoid(x),\n",
    "            'Exponential': torch.exp(x),\n",
    "            'Softplus': F.softplus(x)\n",
    "        }\n",
    "        \n",
    "        print(\"指数函数的优势:\")\n",
    "        print(\"1. 单调递增：相似性越高，概率越大\")\n",
    "        print(\"2. 非负性：exp(x) > 0，适合概率分布\")\n",
    "        print(\"3. 放大差异：小差异被放大，有利于区分\")\n",
    "        print(\"4. 数学优美：与最大熵原理一致\")\n",
    "        print(\"5. 梯度性质：导数等于自身，训练稳定\")\n",
    "        \n",
    "        return functions\n",
    "    \n",
    "    def demonstrate_maximum_entropy_principle(self):\n",
    "        \"\"\"\n",
    "        演示最大熵原理与指数分布的关系\n",
    "        \"\"\"\n",
    "        print(\"\\n=== 4. 最大熵原理 ===\")\n",
    "        \n",
    "        print(\"最大熵原理：在满足约束条件的前提下，选择熵最大的分布\")\n",
    "        print(\"约束条件：E[f(x)] = μ (特征期望等于观测值)\")\n",
    "        print(\"结果：最优分布是指数族分布 p(x) ∝ exp(θ·f(x))\")\n",
    "        print(\"\")\n",
    "        print(\"在Word2Vec中:\")\n",
    "        print(\"- 特征函数 f(w_O, w_I) = v'_O^T * v_I (内积特征)\")\n",
    "        print(\"- 参数 θ = 1\")\n",
    "        print(\"- 因此 p(w_O|w_I) ∝ exp(v'_O^T * v_I)\")\n",
    "        \n",
    "    def demonstrate_practical_benefits(self):\n",
    "        \"\"\"\n",
    "        演示实际训练中的好处\n",
    "        \"\"\"\n",
    "        print(\"\\n=== 5. 实际训练中的好处 ===\")\n",
    "        \n",
    "        # 模拟训练过程\n",
    "        vocab_size = 6\n",
    "        embedding_dim = 4\n",
    "        \n",
    "        # 随机初始化词向量\n",
    "        torch.manual_seed(42)\n",
    "        center_embeds = torch.randn(vocab_size, embedding_dim) * 0.1\n",
    "        context_embeds = torch.randn(vocab_size, embedding_dim) * 0.1\n",
    "        \n",
    "        center_word_id = 0\n",
    "        true_context_id = 1\n",
    "        \n",
    "        print(\"训练前的概率分布:\")\n",
    "        center_vec = center_embeds[center_word_id]\n",
    "        scores = torch.matmul(center_vec, context_embeds.t())\n",
    "        probs_before = F.softmax(scores, dim=0)\n",
    "        \n",
    "        for i in range(vocab_size):\n",
    "            print(f\"词 {i}: p = {probs_before[i]:.4f}\")\n",
    "        \n",
    "        # 模拟梯度更新（简化版）\n",
    "        learning_rate = 0.1\n",
    "        \n",
    "        # 计算梯度并更新\n",
    "        log_probs = F.log_softmax(scores, dim=0)\n",
    "        loss = -log_probs[true_context_id]\n",
    "        \n",
    "        # 手动计算梯度（简化）\n",
    "        grad = probs_before.clone()\n",
    "        grad[true_context_id] -= 1  # softmax的梯度\n",
    "        \n",
    "        # 更新嵌入\n",
    "        for i in range(vocab_size):\n",
    "            context_embeds[i] -= learning_rate * grad[i] * center_vec\n",
    "        \n",
    "        print(f\"\\n训练后的概率分布 (目标词是 {true_context_id}):\")\n",
    "        scores_after = torch.matmul(center_vec, context_embeds.t())\n",
    "        probs_after = F.softmax(scores_after, dim=0)\n",
    "        \n",
    "        for i in range(vocab_size):\n",
    "            change = probs_after[i] - probs_before[i]\n",
    "            print(f\"词 {i}: p = {probs_after[i]:.4f} (变化: {change:+.4f})\")\n",
    "    \n",
    "    def demonstrate_alternative_functions(self):\n",
    "        \"\"\"\n",
    "        展示如果用其他函数会怎样\n",
    "        \"\"\"\n",
    "        print(\"\\n=== 6. 如果用其他函数会怎样？ ===\")\n",
    "        \n",
    "        # 模拟相似性分数\n",
    "        similarities = torch.tensor([0.1, 0.3, 0.8, 0.2, 0.9, 0.4])\n",
    "        \n",
    "        # 不同的概率化方法\n",
    "        methods = {\n",
    "            'Exponential (Word2Vec)': F.softmax(similarities, dim=0),\n",
    "            'Direct normalization': similarities / similarities.sum(),\n",
    "            'Squared': (similarities**2) / (similarities**2).sum(),\n",
    "            'Linear transformed': F.softmax(similarities * 2, dim=0)  # 温度参数\n",
    "        }\n",
    "        \n",
    "        print(\"不同概率化方法的结果:\")\n",
    "        print(\"原始相似性:\", similarities.numpy())\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for method, probs in methods.items():\n",
    "            print(f\"{method:20}: {probs.numpy()}\")\n",
    "        \n",
    "        print(\"\\n指数函数的独特性:\")\n",
    "        print(\"- 保持单调性：相似性排序不变\")\n",
    "        print(\"- 放大差异：高相似性词获得更高概率\") \n",
    "        print(\"- 数学性质：梯度计算简单，训练稳定\")\n",
    "        print(\"- 理论基础：最大熵原理的自然结果\")\n",
    "\n",
    "\n",
    "class GeometricIntuition:\n",
    "    \"\"\"\n",
    "    从几何角度理解内积和指数函数\n",
    "    \"\"\"\n",
    "    \n",
    "    def demonstrate_geometric_meaning(self):\n",
    "        \"\"\"\n",
    "        演示内积的几何意义\n",
    "        \"\"\"\n",
    "        print(\"\\n=== 几何直觉 ===\")\n",
    "        \n",
    "        # 2D示例便于理解\n",
    "        vec_a = torch.tensor([1.0, 0.0])  # 基准向量\n",
    "        vec_b = torch.tensor([0.8, 0.6])  # 30度角\n",
    "        vec_c = torch.tensor([0.0, 1.0])  # 90度角\n",
    "        vec_d = torch.tensor([-0.6, 0.8]) # 135度角\n",
    "        \n",
    "        vectors = {'A(基准)': vec_a, 'B(30°)': vec_b, 'C(90°)': vec_c, 'D(135°)': vec_d}\n",
    "        \n",
    "        print(\"向量间的几何关系:\")\n",
    "        for name, vec in vectors.items():\n",
    "            if name != 'A(基准)':\n",
    "                dot_product = torch.dot(vec_a, vec).item()\n",
    "                angle = torch.acos(torch.clamp(\n",
    "                    F.cosine_similarity(vec_a.unsqueeze(0), vec.unsqueeze(0)), \n",
    "                    -1, 1\n",
    "                )).item() * 180 / np.pi\n",
    "                \n",
    "                print(f\"{name}: 内积={dot_product:.3f}, 夹角={angle:.1f}°\")\n",
    "        \n",
    "        print(\"\\n几何直觉:\")\n",
    "        print(\"- 内积 = |a| * |b| * cos(θ)\")\n",
    "        print(\"- 夹角越小，内积越大，相似性越高\")\n",
    "        print(\"- 指数函数将这种相似性转化为概率\")\n",
    "\n",
    "\n",
    "# 运行所有演示\n",
    "if __name__ == \"__main__\":\n",
    "    demo = ProbabilityIntuitionDemo()\n",
    "    geo_demo = GeometricIntuition()\n",
    "    \n",
    "    # 1. 内积相似性\n",
    "    similarities = demo.demonstrate_inner_product_similarity()\n",
    "    \n",
    "    # 2. 指数变换\n",
    "    raw, exp, probs = demo.demonstrate_exponential_transformation()\n",
    "    \n",
    "    # 3. 为什么指数函数\n",
    "    functions = demo.demonstrate_why_exponential()\n",
    "    \n",
    "    # 4. 最大熵原理\n",
    "    demo.demonstrate_maximum_entropy_principle()\n",
    "    \n",
    "    # 5. 训练好处\n",
    "    demo.demonstrate_practical_benefits()\n",
    "    \n",
    "    # 6. 其他函数对比\n",
    "    demo.demonstrate_alternative_functions()\n",
    "    \n",
    "    # 7. 几何直觉\n",
    "    geo_demo.demonstrate_geometric_meaning()\n",
    "    \n",
    "    print(\"\\n=== 总结 ===\")\n",
    "    print(\"p(w_O|w_I) = exp(v'_O^T * v_I) / Σ exp(v'_w^T * v_I) 的深层意义:\")\n",
    "    print(\"\")\n",
    "    print(\"1. 内积 v'_O^T * v_I:\")\n",
    "    print(\"   - 几何意义：测量向量夹角，夹角越小越相似\")\n",
    "    print(\"   - 语言学意义：捕获词汇间的语义相似性\")\n",
    "    print(\"\")\n",
    "    print(\"2. 指数函数 exp(·):\")\n",
    "    print(\"   - 数学意义：最大熵原理的自然结果\")\n",
    "    print(\"   - 实用意义：放大相似性差异，非负化\")\n",
    "    print(\"   - 训练意义：梯度性质好，数值稳定\")\n",
    "    print(\"\")\n",
    "    print(\"3. 整体 softmax:\")\n",
    "    print(\"   - 归一化：确保是有效的概率分布\")\n",
    "    print(\"   - 竞争性：所有词竞争概率质量\")\n",
    "    print(\"   - 可微性：支持端到端梯度训练\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
