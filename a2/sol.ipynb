{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20922bab",
   "metadata": {},
   "source": [
    "# Solution \n",
    "\n",
    "## Understanding Word2Vec\n",
    "\n",
    "### (b): Computing the Partial Derivative\n",
    "\n",
    "We have:\n",
    "- $J_{\\text{naive-softmax}}(\\mathbf{v}_c, o, \\mathbf{U}) = -\\log P(O = o | C = c)$\n",
    "- $P(O = o | C = c) = \\frac{\\exp(\\mathbf{u}_o^T \\mathbf{v}_c)}{\\sum_{w=1}^{|V|} \\exp(\\mathbf{u}_w^T \\mathbf{v}_c)} = \\hat{y}_o$\n",
    "\n",
    "#### Step 1: Express the loss in terms of softmax\n",
    "$$J = -\\log \\hat{y}_o = -\\log \\frac{\\exp(\\mathbf{u}_o^T \\mathbf{v}_c)}{\\sum_{w=1}^{|V|} \\exp(\\mathbf{u}_w^T \\mathbf{v}_c)}$$\n",
    "\n",
    "$$= -\\mathbf{u}_o^T \\mathbf{v}_c + \\log \\sum_{w=1}^{|V|} \\exp(\\mathbf{u}_w^T \\mathbf{v}_c)$$\n",
    "\n",
    "#### Step 2: Compute the gradient\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{v}_c} = \\frac{\\partial}{\\partial \\mathbf{v}_c}\\left[-\\mathbf{u}_o^T \\mathbf{v}_c + \\log \\sum_{w=1}^{|V|} \\exp(\\mathbf{u}_w^T \\mathbf{v}_c)\\right]$$\n",
    "\n",
    "For the first term:\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{v}_c}(-\\mathbf{u}_o^T \\mathbf{v}_c) = -\\mathbf{u}_o$$\n",
    "\n",
    "For the second term, using chain rule:\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{v}_c}\\log \\sum_{w=1}^{|V|} \\exp(\\mathbf{u}_w^T \\mathbf{v}_c) = \\frac{1}{\\sum_{w=1}^{|V|} \\exp(\\mathbf{u}_w^T \\mathbf{v}_c)} \\cdot \\sum_{w=1}^{|V|} \\exp(\\mathbf{u}_w^T \\mathbf{v}_c) \\mathbf{u}_w$$\n",
    "\n",
    "$$= \\frac{\\sum_{w=1}^{|V|} \\exp(\\mathbf{u}_w^T \\mathbf{v}_c) \\mathbf{u}_w}{\\sum_{w=1}^{|V|} \\exp(\\mathbf{u}_w^T \\mathbf{v}_c)} = \\sum_{w=1}^{|V|} \\frac{\\exp(\\mathbf{u}_w^T \\mathbf{v}_c)}{\\sum_{k=1}^{|V|} \\exp(\\mathbf{u}_k^T \\mathbf{v}_c)} \\mathbf{u}_w$$\n",
    "\n",
    "$$= \\sum_{w=1}^{|V|} \\hat{y}_w \\mathbf{u}_w$$\n",
    "\n",
    "#### Step 3: Combine terms\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{v}_c} = -\\mathbf{u}_o + \\sum_{w=1}^{|V|} \\hat{y}_w \\mathbf{u}_w$$\n",
    "\n",
    "#### Step 4: Express in vectorized form using ð², ð²Ì‚, and ð”\n",
    "\n",
    "Note that:\n",
    "- $\\mathbf{u}_o = \\mathbf{U} \\mathbf{y}$ (since $\\mathbf{y}$ is one-hot with $y_o = 1$)\n",
    "- $\\sum_{w=1}^{|V|} \\hat{y}_w \\mathbf{u}_w = \\mathbf{U} \\hat{\\mathbf{y}}$\n",
    "\n",
    "\n",
    "### ii When is the gradient equal to zero?\n",
    "\n",
    "The gradient equals zero when:\n",
    "$$\\mathbf{U}(\\hat{\\mathbf{y}} - \\mathbf{y}) = \\mathbf{0}$$\n",
    "\n",
    "This occurs when $\\hat{\\mathbf{y}} - \\mathbf{y} = \\mathbf{0}$, i.e., when $\\hat{\\mathbf{y}} = \\mathbf{y}$.\n",
    "\n",
    "**Answer**: The gradient is zero when the predicted probability distribution perfectly matches the true distribution (when $\\hat{y}_o = 1$ and $\\hat{y}_w = 0$ for all $w \\neq o$).\n",
    "\n",
    "#### Interpretation of the gradient terms\n",
    "\n",
    "The gradient can be written as:\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{v}_c} = \\mathbf{U}\\hat{\\mathbf{y}} - \\mathbf{U}\\mathbf{y}$$\n",
    "\n",
    "#### Term 1: $\\mathbf{U}\\hat{\\mathbf{y}}$ (Predicted Context)\n",
    "- This is a weighted average of all word vectors, weighted by their predicted probabilities\n",
    "- It represents the \"expected\" context vector based on the current model's predictions\n",
    "- When subtracted from $\\mathbf{v}_c$, it pulls the center vector away from irrelevant word directions\n",
    "\n",
    "#### Term 2: $-\\mathbf{U}\\mathbf{y}$ (True Context)\n",
    "- This is simply $-\\mathbf{u}_o$, the negative of the true output word vector\n",
    "- When subtracted from $\\mathbf{v}_c$, the double negative means we add $\\mathbf{u}_o$\n",
    "- This pushes the center vector toward the true output word vector\n",
    "\n",
    "#### Combined Effect\n",
    "When we update $\\mathbf{v}_c := \\mathbf{v}_c - \\alpha \\frac{\\partial J}{\\partial \\mathbf{v}_c}$:\n",
    "\n",
    "1. **Attraction**: The center vector is pulled toward the true output word vector ($\\mathbf{u}_o$)\n",
    "2. **Repulsion**: The center vector is pushed away from the weighted average of all word vectors (especially those with high predicted probability but shouldn't be there)\n",
    "\n",
    "This creates the desired effect: making the center vector more similar to the true context word and less similar to incorrect words that the model incorrectly predicts with high probability.\n",
    "\n",
    "\n",
    "## (c): When L2 Normalization Takes Away Useful Information\n",
    "\n",
    "L2 normalization removes useful information when the **magnitude** of word embeddings carries semantic meaning that's relevant to the classification task.\n",
    "\n",
    "Consider the hint: if $\\mathbf{u}_x = \\alpha\\mathbf{u}_y$ where $\\alpha > 0$, then after L2 normalization:\n",
    "- $\\frac{\\mathbf{u}_x}{||\\mathbf{u}_x||_2} = \\frac{\\alpha\\mathbf{u}_y}{||\\alpha\\mathbf{u}_y||_2} = \\frac{\\alpha\\mathbf{u}_y}{|\\alpha|||\\mathbf{u}_y||_2} = \\frac{\\mathbf{u}_y}{||\\mathbf{u}_y||_2}$\n",
    "\n",
    "So normalized $\\mathbf{u}_x$ and normalized $\\mathbf{u}_y$ become **identical**.\n",
    "\n",
    "This is problematic when words have the same semantic direction but different **intensity**. For example:\n",
    "- \"good\" vs \"excellent\": If $\\mathbf{u}_{\\text{excellent}} = 3\\mathbf{u}_{\\text{good}}$, the magnitude difference captures that \"excellent\" is more positive than \"good\"\n",
    "- \"bad\" vs \"terrible\": Similarly, if $\\mathbf{u}_{\\text{terrible}} = 2\\mathbf{u}_{\\text{bad}}$, the magnitude indicates stronger negativity\n",
    "\n",
    "In the classification task, the phrase \"This movie is excellent\" should contribute more positive signal than \"This movie is good\". But after normalization, both words contribute equally, losing the intensity information encoded in the original magnitudes.\n",
    "\n",
    "### When L2 Normalization Doesn't Take Away Useful Information\n",
    "\n",
    "L2 normalization preserves useful information when only the **direction** (semantic meaning) matters, not the magnitude.\n",
    "\n",
    "This occurs when:\n",
    "\n",
    "1. **Magnitude differences are noise**: If embedding magnitudes reflect training artifacts, word frequency, or other non-semantic factors rather than semantic intensity, normalization removes this noise.\n",
    "\n",
    "2. **Words are semantically equivalent**: When $\\mathbf{u}_x = \\alpha\\mathbf{u}_y$ represents true semantic equivalence (like synonyms \"happy\" and \"joyful\"), the magnitude difference might be spurious, and normalization correctly treats them equally.\n",
    "\n",
    "3. **Downstream task is direction-sensitive only**: If the classification boundary depends only on the overall semantic direction of the phrase embedding (sum of individual embeddings), not its magnitude, then normalization can actually improve performance by removing irrelevant scale variations.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Normalization hurts** when magnitude encodes semantic intensity (degree of positivity/negativity)\n",
    "- **Normalization helps** when magnitude represents noise or when only semantic direction matters for the task\n",
    "\n",
    "The key insight is that L2 normalization fundamentally changes the contribution weighting scheme from magnitude-dependent to purely directional, which may or may not align with the semantic structure relevant to your downstream task.\n",
    "\n",
    "## (d): Computing the partial derivatives\n",
    "\n",
    "The loss can be written as:\n",
    "$$J = -\\log \\hat{y}_o = -\\log P(O=o|C=c)$$\n",
    "\n",
    "Taking the partial derivative with respect to $\\mathbf{u}_w$:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{u}_w} = -\\frac{1}{\\hat{y}_o} \\frac{\\partial \\hat{y}_o}{\\partial \\mathbf{u}_w}$$\n",
    "\n",
    "Now I need to compute $\\frac{\\partial \\hat{y}_o}{\\partial \\mathbf{u}_w}$.\n",
    "\n",
    "Since $\\hat{y}_o = \\frac{\\exp(\\mathbf{u}_o^T \\mathbf{v}_c)}{\\sum_{k} \\exp(\\mathbf{u}_k^T \\mathbf{v}_c)}$, using the quotient rule:\n",
    "\n",
    "$$\\frac{\\partial \\hat{y}_o}{\\partial \\mathbf{u}_w} = \\frac{\\frac{\\partial}{\\partial \\mathbf{u}_w}[\\exp(\\mathbf{u}_o^T \\mathbf{v}_c)] \\cdot \\sum_{k} \\exp(\\mathbf{u}_k^T \\mathbf{v}_c) - \\exp(\\mathbf{u}_o^T \\mathbf{v}_c) \\cdot \\frac{\\partial}{\\partial \\mathbf{u}_w}[\\sum_{k} \\exp(\\mathbf{u}_k^T \\mathbf{v}_c)]}{[\\sum_{k} \\exp(\\mathbf{u}_k^T \\mathbf{v}_c)]^2}$$\n",
    "\n",
    "### Case 1: When $w = o$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{u}_o}[\\exp(\\mathbf{u}_o^T \\mathbf{v}_c)] = \\exp(\\mathbf{u}_o^T \\mathbf{v}_c) \\mathbf{v}_c$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{u}_o}[\\sum_{k} \\exp(\\mathbf{u}_k^T \\mathbf{v}_c)] = \\exp(\\mathbf{u}_o^T \\mathbf{v}_c) \\mathbf{v}_c$$\n",
    "\n",
    "Therefore:\n",
    "$$\\frac{\\partial \\hat{y}_o}{\\partial \\mathbf{u}_o} = \\frac{\\exp(\\mathbf{u}_o^T \\mathbf{v}_c) \\mathbf{v}_c \\cdot \\sum_{k} \\exp(\\mathbf{u}_k^T \\mathbf{v}_c) - \\exp(\\mathbf{u}_o^T \\mathbf{v}_c) \\cdot \\exp(\\mathbf{u}_o^T \\mathbf{v}_c) \\mathbf{v}_c}{[\\sum_{k} \\exp(\\mathbf{u}_k^T \\mathbf{v}_c)]^2}$$\n",
    "\n",
    "$$= \\frac{\\exp(\\mathbf{u}_o^T \\mathbf{v}_c) \\mathbf{v}_c [\\sum_{k} \\exp(\\mathbf{u}_k^T \\mathbf{v}_c) - \\exp(\\mathbf{u}_o^T \\mathbf{v}_c)]}{[\\sum_{k} \\exp(\\mathbf{u}_k^T \\mathbf{v}_c)]^2}$$\n",
    "\n",
    "$$= \\hat{y}_o \\mathbf{v}_c (1 - \\hat{y}_o)$$\n",
    "\n",
    "So: $$\\frac{\\partial J}{\\partial \\mathbf{u}_o} = -\\frac{1}{\\hat{y}_o} \\cdot \\hat{y}_o \\mathbf{v}_c (1 - \\hat{y}_o) = -\\mathbf{v}_c (1 - \\hat{y}_o) = \\mathbf{v}_c(\\hat{y}_o - 1)$$\n",
    "\n",
    "Since $y_o = 1$:\n",
    "$$\\boxed{\\frac{\\partial J}{\\partial \\mathbf{u}_o} = \\mathbf{v}_c(\\hat{y}_o - y_o)}$$\n",
    "\n",
    "### Case 2: When $w \\neq o$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{u}_w}[\\exp(\\mathbf{u}_o^T \\mathbf{v}_c)] = 0$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{u}_w}[\\sum_{k} \\exp(\\mathbf{u}_k^T \\mathbf{v}_c)] = \\exp(\\mathbf{u}_w^T \\mathbf{v}_c) \\mathbf{v}_c$$\n",
    "\n",
    "Therefore:\n",
    "$$\\frac{\\partial \\hat{y}_o}{\\partial \\mathbf{u}_w} = \\frac{0 - \\exp(\\mathbf{u}_o^T \\mathbf{v}_c) \\cdot \\exp(\\mathbf{u}_w^T \\mathbf{v}_c) \\mathbf{v}_c}{[\\sum_{k} \\exp(\\mathbf{u}_k^T \\mathbf{v}_c)]^2}$$\n",
    "\n",
    "$$= -\\frac{\\exp(\\mathbf{u}_o^T \\mathbf{v}_c)}{\\sum_{k} \\exp(\\mathbf{u}_k^T \\mathbf{v}_c)} \\cdot \\frac{\\exp(\\mathbf{u}_w^T \\mathbf{v}_c)}{\\sum_{k} \\exp(\\mathbf{u}_k^T \\mathbf{v}_c)} \\mathbf{v}_c$$\n",
    "\n",
    "$$= -\\hat{y}_o \\hat{y}_w \\mathbf{v}_c$$\n",
    "\n",
    "So: $$\\frac{\\partial J}{\\partial \\mathbf{u}_w} = -\\frac{1}{\\hat{y}_o} \\cdot (-\\hat{y}_o \\hat{y}_w \\mathbf{v}_c) = \\hat{y}_w \\mathbf{v}_c$$\n",
    "\n",
    "Since $y_w = 0$ for $w \\neq o$:\n",
    "$$oxed{\\frac{\\partial J}{\\partial \\mathbf{u}_w} = \\mathbf{v}_c(\\hat{y}_w - y_w) \\text{ for } w \\neq o}$$\n",
    "\n",
    "## Final Answer\n",
    "\n",
    "For both cases, we can write the unified expression:\n",
    "\n",
    "$$oxed{\\frac{\\partial J}{\\partial \\mathbf{u}_w} = \\mathbf{v}_c(\\hat{y}_w - y_w) \\text{ for all } w}$$\n",
    "\n",
    "where:\n",
    "- When $w = o$: $y_w = y_o = 1$\n",
    "- When $w \\neq o$: $y_w = 0$\n",
    "\n",
    "Looking at the structure of the matrix **U** and the partial derivatives I computed in the previous part, I need to arrange the column vector derivatives to form the gradient matrix.\n",
    "\n",
    "Since **U** is a matrix where each column is an outside word vector:\n",
    "$$\\mathbf{U} = [\\mathbf{u}_1, \\mathbf{u}_2, \\ldots, \\mathbf{u}_{|\\text{Vocab}|}]$$\n",
    "\n",
    "The partial derivative with respect to **U** will have the same structure, where each column is the partial derivative with respect to the corresponding column vector of **U**.\n",
    "\n",
    "From the previous part, I found that:\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{u}_w} = \\mathbf{v}_c(\\hat{y}_w - y_w) \\text{ for all } w$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$oxed{\\frac{\\partial J(\\mathbf{v}_c, o, \\mathbf{U})}{\\partial \\mathbf{U}} = \\left[\\frac{\\partial J(\\mathbf{v}_c, o, \\mathbf{U})}{\\partial \\mathbf{u}_1}, \\frac{\\partial J(\\mathbf{v}_c, o, \\mathbf{U})}{\\partial \\mathbf{u}_2}, \\ldots, \\frac{\\partial J(\\mathbf{v}_c, o, \\mathbf{U})}{\\partial \\mathbf{u}_{|\\text{Vocab}|}}\\right]}$$\n",
    "\n",
    "This can be written more compactly as:\n",
    "\n",
    "$$\\boxed{\\frac{\\partial J(\\mathbf{v}_c, o, \\mathbf{U})}{\\partial \\mathbf{U}} = \\mathbf{v}_c(\\hat{\\mathbf{y}} - \\mathbf{y})^T}$$\n",
    "\n",
    "**Explanation:** The gradient matrix has the same dimensions as **U**. Each column corresponds to the gradient with respect to the corresponding outside word vector. The compact form shows that we can compute this as the outer product of the center vector $\\mathbf{v}_c$ with the difference between predicted and true probability distributions $(\\hat{\\mathbf{y}} - \\mathbf{y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0fc2e",
   "metadata": {},
   "source": [
    "## Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc11b904",
   "metadata": {},
   "source": [
    "### a-i \n",
    "\n",
    "Looking at the momentum mechanism in Adam optimization, the key insight is that $\\mathbf{m_{t+1}}$ represents an exponentially weighted moving average of past gradients rather than just using the current gradient directly.\n",
    "\n",
    "This rolling average has the effect of smoothing out the gradient updates by dampening oscillations and noise that occur from minibatch to minibatch. When gradients consistently point in the same direction across multiple steps, the momentum term amplifies this signal, while when gradients fluctuate randomly (due to noisy minibatches or local curvature), the averaging effect reduces these fluctuations.\n",
    "\n",
    "This lower variance in updates is helpful for learning because it allows the optimizer to make more consistent progress toward the optimum without getting sidetracked by noisy or conflicting gradient signals. The momentum helps the optimizer \"remember\" the general direction it should be moving and prevents it from making erratic updates that could slow convergence or cause the training to become unstable.\n",
    "\n",
    "### a-ii\n",
    "\n",
    "Looking at the Adam update rule, the parameters that will get **larger updates** are those with **smaller values of $\\sqrt{v_{t+1}}$**.\n",
    "\n",
    "Since $\\mathbf{v_{t+1}}$ tracks a rolling average of the squared gradients (gradient magnitudes), parameters that have historically had **small gradients** will have small values in $v_{t+1}$, leading to small values of $\\sqrt{v_{t+1}}$, and therefore **larger effective learning rates** when we divide by $\\sqrt{v_{t+1}}$.\n",
    "\n",
    "Conversely, parameters that have historically had **large gradients** will have large values in $v_{t+1}$, leading to **smaller effective learning rates**.\n",
    "\n",
    "This adaptive mechanism helps with learning because it addresses the problem of having a single global learning rate for all parameters. In many neural networks, different parameters naturally have very different gradient scales - some parameters might consistently receive tiny gradients while others receive large gradients. With a fixed learning rate, parameters with small gradients would update too slowly (potentially getting stuck), while parameters with large gradients might update too aggressively (potentially overshooting).\n",
    "\n",
    "By giving larger effective learning rates to parameters with historically small gradients and smaller effective learning rates to parameters with historically large gradients, Adam automatically balances the update magnitudes across different parameters, allowing for more efficient and stable learning across the entire parameter space.\n",
    "\n",
    "\n",
    "### b-i\n",
    "\n",
    "### b-ii\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711ec9b9",
   "metadata": {},
   "source": [
    "## Neural Transition-Based Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8af8c1",
   "metadata": {},
   "source": [
    "## a\n",
    "\n",
    "| Stack | Buffer | New dependency | Transition |\n",
    "|-------|--------|----------------|------------|\n",
    "| [ROOT] | [I, presented, my, findings, at, the, NLP, conference] |  | Initial Configuration |\n",
    "| [ROOT, I] | [presented, my, findings, at, the, NLP, conference] |  | SHIFT |\n",
    "| [ROOT, I, presented] | [my, findings, at, the, NLP, conference] |  | SHIFT |\n",
    "| [ROOT, presented] | [my, findings, at, the, NLP, conference] | presentedâ†’I | LEFT-ARC |\n",
    "| [ROOT, presented, my] | [findings, at, the, NLP, conference] |  | SHIFT |\n",
    "| [ROOT, presented, my, findings] | [at, the, NLP, conference] |  | SHIFT |\n",
    "| [ROOT, presented, findings] | [at, the, NLP, conference] | findingsâ†’my | LEFT-ARC |\n",
    "| [ROOT, presented, findings, at] | [the, NLP, conference] |  | SHIFT |\n",
    "| [ROOT, presented, findings, at, the] | [NLP, conference] |  | SHIFT |\n",
    "| [ROOT, presented, findings, at, the, NLP] | [conference] |  | SHIFT |\n",
    "| [ROOT, presented, findings, at, the, NLP, conference] | [] |  | SHIFT |\n",
    "| [ROOT, presented, findings, at, the, conference] | [] | conferenceâ†’NLP | LEFT-ARC |\n",
    "| [ROOT, presented, findings, at, conference] | [] | conferenceâ†’the | LEFT-ARC |\n",
    "| [ROOT, presented, findings, at] | [] | atâ†’conference | RIGHT-ARC |\n",
    "| [ROOT, presented, findings] | [] | findingsâ†’at | RIGHT-ARC |\n",
    "| [ROOT, presented] | [] | presentedâ†’findings | RIGHT-ARC |\n",
    "| [ROOT] | [] | ROOTâ†’presented | RIGHT-ARC |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b27c8f",
   "metadata": {},
   "source": [
    "## b\n",
    "\n",
    "2n\n",
    "\n",
    "explanation: n shift + n Arc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39f0a9",
   "metadata": {},
   "source": [
    "## e-i\n",
    "\n",
    "I'll compute the derivative of $\\mathbf{h} = \\text{ReLU}(\\mathbf{x} \\mathbf{W} + \\mathbf{b}_1)$ with respect to $\\mathbf{x}$.\n",
    "\n",
    "Let me break this down step by step.\n",
    "\n",
    "First, let's establish the setup:\n",
    "- $\\mathbf{x} \\in \\mathbb{R}^{dm}$ is the input vector\n",
    "- $\\mathbf{W} \\in \\mathbb{R}^{dm \\times h}$ is the weight matrix (where $h$ is the hidden layer size)  \n",
    "- $\\mathbf{b}_1 \\in \\mathbb{R}^h$ is the bias vector\n",
    "- $\\mathbf{h} \\in \\mathbb{R}^h$ is the hidden layer output\n",
    "\n",
    "The computation proceeds as:\n",
    "1. $\\mathbf{z} = \\mathbf{x} \\mathbf{W} + \\mathbf{b}_1$ (linear transformation)\n",
    "2. $\\mathbf{h} = \\text{ReLU}(\\mathbf{z})$ (element-wise ReLU activation)\n",
    "\n",
    "For the $i$-th component of $\\mathbf{h}$:\n",
    "$$h_i = \\text{ReLU}(z_i) = \\text{ReLU}\\left(\\sum_{k=1}^{dm} x_k W_{k,i} + b_{1,i}\\right)$$\n",
    "\n",
    "where $z_i = \\sum_{k=1}^{dm} x_k W_{k,i} + b_{1,i}$.\n",
    "\n",
    "Now, to find $\\frac{\\partial h_i}{\\partial x_j}$:\n",
    "\n",
    "Using the chain rule:\n",
    "$$\\frac{\\partial h_i}{\\partial x_j} = \\frac{\\partial h_i}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial x_j}$$\n",
    "\n",
    "Computing each part:\n",
    "\n",
    "1. $\\frac{\\partial z_i}{\\partial x_j} = W_{j,i}$ (since $z_i = \\sum_{k=1}^{dm} x_k W_{k,i} + b_{1,i}$)\n",
    "\n",
    "2. $\\frac{\\partial h_i}{\\partial z_i} = \\frac{\\partial}{\\partial z_i} \\text{ReLU}(z_i) = \\begin{cases} \n",
    "   1 & \\text{if } z_i > 0 \\\\\n",
    "   0 & \\text{if } z_i < 0\n",
    "   \\end{cases}$\n",
    "\n",
    "Therefore:\n",
    "$$\\frac{\\partial h_i}{\\partial x_j} = \\begin{cases} \n",
    "W_{j,i} & \\text{if } z_i > 0 \\\\\n",
    "0 & \\text{if } z_i < 0\n",
    "\\end{cases}$$\n",
    "\n",
    "This can also be written more compactly as:\n",
    "$$\\frac{\\partial h_i}{\\partial x_j} = W_{j,i} \\cdot \\mathbf{1}[z_i > 0]$$\n",
    "\n",
    "where $\\mathbf{1}[z_i > 0]$ is the indicator function that equals 1 when $z_i > 0$ and 0 otherwise.\n",
    "\n",
    "The key insight is that the derivative depends on whether the pre-activation $z_i$ is positive or negative. When $z_i > 0$, the ReLU is in its linear region and the gradient flows through with weight $W_{j,i}$. When $z_i < 0$, the ReLU saturates at 0 and blocks the gradient completely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b450568",
   "metadata": {},
   "source": [
    "I'll compute the partial derivative of the cross-entropy loss $CE(\\mathbf{y}, \\hat{\\mathbf{y}})$ with respect to $\\mathbf{l}_i$.\n",
    "\n",
    "Given:\n",
    "- $\\mathbf{l} \\in \\mathbb{R}^3$ (logits)\n",
    "- $\\hat{\\mathbf{y}} \\in \\mathbb{R}^3$ (predictions from softmax)\n",
    "- $\\mathbf{y} \\in \\mathbb{R}^3$ (true one-hot label)\n",
    "- True label is $c$ (so $\\mathbf{y}_c = 1$ and $\\mathbf{y}_j = 0$ for $j \\neq c$)\n",
    "\n",
    "The cross-entropy loss is:\n",
    "$$CE(\\mathbf{y}, \\hat{\\mathbf{y}}) = -\\sum_{j=1}^{3} \\mathbf{y}_j \\log \\hat{\\mathbf{y}}_j = -\\mathbf{y}_c \\log \\hat{\\mathbf{y}}_c = -\\log \\hat{\\mathbf{y}}_c$$\n",
    "\n",
    "The softmax function gives us:\n",
    "$$\\hat{\\mathbf{y}}_j = \\frac{\\exp(\\mathbf{l}_j)}{\\sum_{k=1}^{3} \\exp(\\mathbf{l}_k)}$$\n",
    "\n",
    "Using the chain rule as suggested in the hint:\n",
    "$$\\frac{\\partial CE(\\mathbf{y}, \\hat{\\mathbf{y}})}{\\partial \\mathbf{l}_i} = \\sum_{j=1}^{3} \\frac{\\partial CE(\\mathbf{y}, \\hat{\\mathbf{y}})}{\\partial \\hat{\\mathbf{y}}_j} \\frac{\\partial \\hat{\\mathbf{y}}_j}{\\partial \\mathbf{l}_i}$$\n",
    "\n",
    "First, let's compute $\\frac{\\partial CE(\\mathbf{y}, \\hat{\\mathbf{y}})}{\\partial \\hat{\\mathbf{y}}_j}$:\n",
    "\n",
    "Since $CE(\\mathbf{y}, \\hat{\\mathbf{y}}) = -\\log \\hat{\\mathbf{y}}_c$:\n",
    "$$\\frac{\\partial CE(\\mathbf{y}, \\hat{\\mathbf{y}})}{\\partial \\hat{\\mathbf{y}}_j} = \\begin{cases}\n",
    "-\\frac{1}{\\hat{\\mathbf{y}}_c} & \\text{if } j = c \\\\\n",
    "0 & \\text{if } j \\neq c\n",
    "\\end{cases}$$\n",
    "\n",
    "This confirms the hint that $\\frac{\\partial CE(\\mathbf{y}, \\hat{\\mathbf{y}})}{\\partial \\hat{\\mathbf{y}}_j} = 0$ if $j \\neq c$.\n",
    "\n",
    "Therefore, the sum reduces to:\n",
    "$$\\frac{\\partial CE(\\mathbf{y}, \\hat{\\mathbf{y}})}{\\partial \\mathbf{l}_i} = \\frac{\\partial CE(\\mathbf{y}, \\hat{\\mathbf{y}})}{\\partial \\hat{\\mathbf{y}}_c} \\frac{\\partial \\hat{\\mathbf{y}}_c}{\\partial \\mathbf{l}_i} = -\\frac{1}{\\hat{\\mathbf{y}}_c} \\frac{\\partial \\hat{\\mathbf{y}}_c}{\\partial \\mathbf{l}_i}$$\n",
    "\n",
    "Now I need to compute $\\frac{\\partial \\hat{\\mathbf{y}}_c}{\\partial \\mathbf{l}_i}$ using the softmax derivative:\n",
    "\n",
    "For softmax $\\hat{\\mathbf{y}}_j = \\frac{\\exp(\\mathbf{l}_j)}{\\sum_{k=1}^{3} \\exp(\\mathbf{l}_k)}$:\n",
    "\n",
    "$$\\frac{\\partial \\hat{\\mathbf{y}}_c}{\\partial \\mathbf{l}_i} = \\begin{cases}\n",
    "\\hat{\\mathbf{y}}_c(1 - \\hat{\\mathbf{y}}_c) & \\text{if } i = c \\\\\n",
    "-\\hat{\\mathbf{y}}_c \\hat{\\mathbf{y}}_i & \\text{if } i \\neq c\n",
    "\\end{cases}$$\n",
    "\n",
    "Substituting back:\n",
    "\n",
    "**Case 1: $i = c$**\n",
    "$$\\frac{\\partial CE(\\mathbf{y}, \\hat{\\mathbf{y}})}{\\partial \\mathbf{l}_i} = -\\frac{1}{\\hat{\\mathbf{y}}_c} \\cdot \\hat{\\mathbf{y}}_c(1 - \\hat{\\mathbf{y}}_c) = -(1 - \\hat{\\mathbf{y}}_c) = \\hat{\\mathbf{y}}_c - 1$$\n",
    "\n",
    "**Case 2: $i \\neq c$**\n",
    "$$\\frac{\\partial CE(\\mathbf{y}, \\hat{\\mathbf{y}})}{\\partial \\mathbf{l}_i} = -\\frac{1}{\\hat{\\mathbf{y}}_c} \\cdot (-\\hat{\\mathbf{y}}_c \\hat{\\mathbf{y}}_i) = \\hat{\\mathbf{y}}_i$$\n",
    "\n",
    "Therefore:\n",
    "$$\\frac{\\partial CE(\\mathbf{y}, \\hat{\\mathbf{y}})}{\\partial \\mathbf{l}_i} = \\begin{cases}\n",
    "\\hat{\\mathbf{y}}_c - 1 & \\text{if } i = c \\\\\n",
    "\\hat{\\mathbf{y}}_i & \\text{if } i \\neq c\n",
    "\\end{cases}$$\n",
    "\n",
    "This can be written more compactly as:\n",
    "$$\\frac{\\partial CE(\\mathbf{y}, \\hat{\\mathbf{y}})}{\\partial \\mathbf{l}_i} = \\hat{\\mathbf{y}}_i - \\mathbf{y}_i$$\n",
    "\n",
    "where $\\mathbf{y}_i$ is the $i$-th component of the one-hot true label vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ee36e0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
